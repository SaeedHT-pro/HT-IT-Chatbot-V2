# app3.py
import streamlit as st
from chatbot_utils import (
    get_gemini_llm,
    get_embedding_model,
    get_combined_retriever,
    perform_duckduckgo_search,
    INITIAL_ANALYSIS_PROMPT_TEMPLATE,
    RESPONSE_GENERATION_PROMPT_TEMPLATE,
    RELEVANCE_CHECK_PROMPT_TEMPLATE,
    clean_json_response,
    fetch_url_title,
    logger
)
import time
import re

# --- HELPER FUNCTION FOR LINK PREVIEWS (Copied from app2.py) ---
def process_response_for_link_previews(text: str) -> str:
    logger.debug(f"Starting link preview processing for text: {text[:100]}...")
    preview_link_pattern = r'\[PREVIEW\]\(([^)]+)\)'
    def replace_link(match):
        url = match.group(1)
        logger.info(f"Found [PREVIEW] link for URL: {url}")
        media_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.mp4', '.mp3', '.pdf', '.zip', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
        if any(url.lower().endswith(ext) for ext in media_extensions):
            logger.info(f"URL {url} appears to be a direct media link. Skipping title fetch, using URL as title.")
            return f"[{url.split('/')[-1]}]({url})"
        title = fetch_url_title(url)
        if title and title != url:
            title = title.replace("[", "\\[").replace("]", "\\]")
            logger.info(f"Replacing [PREVIEW]({url}) with title: [{title}]({url})")
            return f"[{title}]({url})"
        else:
            logger.info(f"Failed to fetch a distinct title for {url}. Using URL as link text.")
            return f"[{url}]({url})"
    processed_text = re.sub(preview_link_pattern, replace_link, text)
    if processed_text != text:
        logger.debug(f"Link preview processing completed. Text was modified.")
    else:
        logger.debug(f"Link preview processing completed. No [PREVIEW] links found or no changes made.")
    return processed_text

# --- PAGE CONFIG ---
st.set_page_config(page_title="IT Support Chatbot", layout="wide")
st.title("IT Support Chatbot Assistant ðŸ¤–") # Ensuring emoji is here
# st.caption("Powered by Gemini, FAISS, LangChain & Streamlit") # Caption can be page level or column specific

# --- INITIALIZATION & CACHING (Global for the app) ---
@st.cache_resource
def load_models_and_retrievers():
    logger.info("Attempting to load models and retrievers for Streamlit session...")
    llm = get_gemini_llm()
    embedding_model = get_embedding_model()
    force_recreate_indexes = False # Default
    combined_retriever = get_combined_retriever(embedding_model, force_recreate=force_recreate_indexes)
    logger.info("Models and retrievers loading complete for Streamlit session.")
    return llm, combined_retriever

llm, combined_retriever = load_models_and_retrievers()

# --- DEFINE COLUMNS ---
left_column, right_column = st.columns([2, 1]) # Left column twice as wide as the right

# --- CONTENT FOR THE LEFT COLUMN ---
with left_column:
    st.header("Main Application Area")
    st.write("This area can be used for other application features or content.")
    st.write("For example, in the screenshot you shared, this side had an 'Intents' builder.")
    st.image("https://via.placeholder.com/600x400.png?text=Main+Content+Placeholder", caption="Placeholder for Main Content")

# --- CONTENT FOR THE RIGHT COLUMN (Chatbot) ---
with right_column:
    st.header("Chatbot")

    # Inject custom CSS for a fixed-height, scrollable chat area.
    # IMPORTANT: The CSS selectors used below are EXAMPLES and LIKELY WONT WORK directly.
    # You'll need to inspect the HTML generated by Streamlit in your browser's
    # developer tools to find the correct selectors for the elements you want to style.
    # Streamlit's internal HTML structure can change, making these selectors fragile.
    
    # The goal is to make the 'message_area_container' scrollable and have a fixed height,
    # and ensure the chat_input stays at the bottom of the right_column.
    # This often involves making the parent of message_area_container and chat_input a flex column.
    
    # Get the parent div of the column: Streamlit columns are often stVerticalBlock within stHorizontalBlock's children.
    # This is a common pattern but can vary.
    # Example: Targeting the specific vertical block of the right column.
    # If right_column is the second column (index 1):
    # Assume parent of columns is .stHorizontalBlock
    # Assume each column is a div child of that
    # Assume content within column is .stVerticalBlock
    
    # This CSS is illustrative and for guidance.
    st.markdown("""
    <style>
    /* This selector targets the specific Streamlit block for the right column.
       You MUST inspect your app's HTML to find the correct selector.
       It might be something like:
       div[data-testid="stVerticalBlock"]:has(div[data-testid="stChatInput"]) 
       or based on nth-child if the column structure is stable.
       For this example, let's imagine the right column's content wrapper has a class we could target
       or is the second child of a known parent.
    */

    /* Make the right column's content area a flex container */
    /* This is a generic selector for a column, likely needs to be more specific */
    /* For example, if your columns are direct children of a 'stHorizontalBlock':
       div.stHorizontalBlock > div:nth-child(2) > div.stVerticalBlock { 
    */
    /* Replace '.right-column-wrapper-selector' with the actual selector for the right column's content area */
    .right-column-wrapper-selector {
        display: flex;
        flex-direction: column;
        height: calc(100vh - 100px); /* Example: Full viewport height minus some offset for headers etc. Adjust as needed. */
    }

    /* Style for the container holding the chat messages */
    /* Replace '.message-area-container-selector' with the actual selector for the st.container below */
    .message-area-container-selector {
        flex-grow: 1; /* Allows the message area to take up available space */
        overflow-y: auto; /* Enables vertical scrolling */
        padding: 10px;
        border: 1px solid #e6e6e6; /* Optional: to see the box */
    }
    </style>
    """, unsafe_allow_html=True)

    # Container for chat messages - this is what we want to be scrollable
    # You would try to target the div generated by this st.container() with your CSS.
    message_area_container = st.container()
    with message_area_container:
        if "messages_app3" not in st.session_state:
            st.session_state.messages_app3 = [{"role": "assistant", "content": "Hi! I'm your IT Support Assistant. How can I help you today?"}]

        for message in st.session_state.messages_app3:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    # CHAT INPUT - Should be a direct child of the flex container (right_column_wrapper)
    # and will be pushed to the bottom by flex-grow on the message_area_container.
    # --- CHAT INPUT AND PROCESSING LOGIC ---
    # st.chat_input is designed to appear at the bottom of the screen or its container.
    # The processing block below handles new messages.
    if user_query := st.chat_input("Ask your IT question..."):
        # Add user message to session state. It will be displayed by the loop above on rerun.
        st.session_state.messages_app3.append({"role": "user", "content": user_query})

        # --- Assistant Response Logic ---
        # Note: The "thinking" indicator and step-by-step UI updates are removed for this st.rerun() approach.
        # If those are critical, a more complex solution involving st.empty() outside this `if` block might be needed.
        
        logger.info(f"--- New User Query Processing Started (app3) ---")
        logger.info(f"User Query: {user_query}")

        # 1. Initial Analysis with LLM
        analysis_prompt_filled = INITIAL_ANALYSIS_PROMPT_TEMPLATE.format(user_query=user_query)
        llm_decision_best_source = "Internal_Docs" # Default
        simplified_query = user_query
        # assistant_reasoning_steps_for_ui = "" # Not directly displayed with rerun model

        try:
            analysis_response = llm.generate_content(analysis_prompt_filled)
            analysis_result_json = clean_json_response(analysis_response.text)
            if analysis_result_json:
                llm_decision_best_source = analysis_result_json.get("best_source", "Internal_Docs")
                simplified_query = analysis_result_json.get("simplified_query_for_search", user_query)
            # ui_source_display = "Internal Documents" if llm_decision_best_source == "Internal_Docs" else llm_decision_best_source.replace('_', ' ')
            # assistant_reasoning_steps_for_ui += f"*(LLM suggests checking: {ui_source_display} first using query: '{simplified_query}')*\n\n"
            else:
                logger.warning("LLM analysis JSON parsing failed.")
        except Exception as e:
            logger.error(f"Error in LLM initial analysis: {e}", exc_info=True)
            # assistant_reasoning_steps_for_ui += f"*(Error during analysis phase: {e}. Defaulting to Internal Documents.)*\n\n"
            # Consider adding error message to chat via session_state if needed
            st.session_state.messages_app3.append({"role": "assistant", "content": f"Error during analysis: {e}"})
            st.rerun()


        # 2. Retrieve Context
        retrieved_context_str = ""
        source_type_used_for_response_prompt = "" 
        relevant_internal_docs_found = False

        def check_context_relevance(query_for_relevance, simplified_search_query, context_to_check, llm_model):
            if not context_to_check: return False
            relevance_prompt_filled = RELEVANCE_CHECK_PROMPT_TEMPLATE.format(
                user_query=query_for_relevance, simplified_query=simplified_search_query, retrieved_context=context_to_check[:3000]
            )
            try:
                relevance_response = llm_model.generate_content(relevance_prompt_filled)
                return "YES" in relevance_response.text.strip().upper()
            except Exception: return False

        if llm_decision_best_source == "Internal_Docs":
            source_type_used_for_response_prompt = "Internal Documents"
            if combined_retriever:
                try:
                    original_k = combined_retriever.search_kwargs.get('k', 3)
                    combined_retriever.search_kwargs['k'] = 5
                    retrieved_docs_list = combined_retriever.get_relevant_documents(simplified_query)
                    combined_retriever.search_kwargs['k'] = original_k
                    if retrieved_docs_list:
                        potential_context_parts = [f"Source: {d.metadata.get('source', 'N/A')}, Type: {d.metadata.get('doc_type', 'N/A')}\nContent: {d.page_content}" for d in retrieved_docs_list]
                        potential_context = "\n\n---\n\n".join(potential_context_parts)
                        if check_context_relevance(user_query, simplified_query, potential_context, llm):
                            retrieved_context_str = potential_context
                            relevant_internal_docs_found = True
                except Exception as e:
                    logger.warning(f"Error retrieving from {source_type_used_for_response_prompt}: {e}", exc_info=True)
            else:
                logger.warning("Combined retriever was None during search attempt.")

        if llm_decision_best_source == "Web_Search" or not relevant_internal_docs_found:
            source_type_used_for_response_prompt = "Web Search"
            web_search_results = perform_duckduckgo_search(simplified_query)
            if web_search_results and "did not yield specific results" not in web_search_results and "failed" not in web_search_results:
                retrieved_context_str = web_search_results
            elif not retrieved_context_str: # Only if internal search also found nothing
                retrieved_context_str = "No relevant information found in internal documents or through web search."
                source_type_used_for_response_prompt = "any available source"
        
        # 3. Generate Final Response with LLM
        final_answer_for_ui = "Sorry, I encountered an issue and couldn't generate a response."
        if not retrieved_context_str or "No relevant information found" in retrieved_context_str:
            final_answer_for_ui = "I couldn't find specific information for your query. Could you rephrase or ask something else?"
        else:
            final_prompt_filled = RESPONSE_GENERATION_PROMPT_TEMPLATE.format(
                user_query=user_query,
                source_type_used=source_type_used_for_response_prompt if source_type_used_for_response_prompt else "available knowledge",
                context=retrieved_context_str
            )
            try:
                final_gemini_response = llm.generate_content(final_prompt_filled)
                final_answer_for_ui = final_gemini_response.text
            except Exception as e:
                logger.error(f"Error generating final response with Gemini: {e}", exc_info=True)
                final_answer_for_ui = f"Sorry, an error occurred while generating response: {e}"

        processed_final_answer_for_ui = process_response_for_link_previews(final_answer_for_ui)
        st.session_state.messages_app3.append({"role": "assistant", "content": processed_final_answer_for_ui})
        logger.info(f"--- User Query Processing Ended (app3) ---")
        st.rerun()
